{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXhWhgWBAtuWH+lZlvCxrx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdi2357/CoverLetter/blob/main/CoverLetrrer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "hTAXNfaoRqDr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bdi2357/CoverLetter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0bLtjxrR_Ya",
        "outputId": "f0a498f8-a87f-4bab-b178-a5a659c3e863"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CoverLetter'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 27 (delta 11), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (27/27), 86.56 KiB | 16.00 KiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "At05GNhPPQ-s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q PyPDF2"
      ],
      "metadata": {
        "id": "4qshbooyX7EJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ccc7c79-81d6-4352-a361-731da343930a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/232.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/232.6 kB\u001b[0m \u001b[31m903.5 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/232.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx\n",
        "!pip install fpdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLepXwZwYBfz",
        "outputId": "7138e322-8ca8-4256-8c80-af7fd2db547b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m51.2/54.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from docx) (4.9.4)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.10/dist-packages (from docx) (9.4.0)\n",
            "Building wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53892 sha256=a64c83d608dfa0f15d85cba8c4a8df9a0dfc77ad69d49ba4295b222d0bf0ccc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/f5/1d/e09ba2c1907a43a4146d1189ae4733ca1a3bfe27ee39507767\n",
            "Successfully built docx\n",
            "Installing collected packages: docx\n",
            "Successfully installed docx-0.2.4\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: fpdf\n",
            "  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=516d476f03922f6d9ea4eda9799024ccb739518628e93daf7968f7e39990ea35\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "Successfully built fpdf\n",
            "Installing collected packages: fpdf\n",
            "Successfully installed fpdf-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIHuTeqilm84",
        "outputId": "5db820d8-fee4-4806-9756-dbcc4f4a6d98"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from genai import GenerativeModel\n",
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "import re\n",
        "import time"
      ],
      "metadata": {
        "id": "a1sqyTAmRfRw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "eB61SnIWVKVm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "-hpD7lN6K1O2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "eyq9gB2IV8E7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-flash')"
      ],
      "metadata": {
        "id": "OdyzB6kgYN9b"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def load_and_extract_text(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        pdf_reader = PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text"
      ],
      "metadata": {
        "id": "scs-j7SadNPa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "yrZFX_ZGCAKu"
      },
      "outputs": [],
      "source": [
        "def generate_cover_letter(model,cv_text, job_description_text):\n",
        "\n",
        "    prompt = f\"\"\"Based on the provided job description and CV, write a concise, compelling and personalized cover letter that highlights relevant skills and experiences.\n",
        "\n",
        "    Job Description:\\n{job_description_text}\\n\\nCV:\\n{cv_text}\\n\\n\n",
        "\n",
        "    Tailor the letter to the specific job requirements and showcase the candidate's match for the position. Be sure to mention specific accomplishments and quantify results whenever possible. Keep the tone professional and precise based on the cv.\n",
        "    As concise as possible no more than 4 sentences. Avoid using adjectives unless neccessary , as less as possible , do not use emotive adjectives in any case, use only factual information based on the cv_text , finalize with best regards, Name where the name of the applicant is taken from the CV\"\"\"\n",
        "    print(\"HERE\")\n",
        "    response = model.generate_content(prompt)  # Remove the 'context' argument\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_description = \"\"\" We're seeking an AI Developer to join our team. In this role, you'll leverage artificial intelligence and machine learning techniques to improve the invoice reconciliation process and create a unified data format across various financial systems.\n",
        "\n",
        "Responsibilities:\n",
        "- Develop and implement AI and machine learning models to automate invoice reconciliation\n",
        "- Create intelligent systems to unify diverse invoice data into a standardized format\n",
        "- Design and build natural language processing (NLP) solutions to extract key information from unstructured invoice data\n",
        "- Implement machine learning algorithms to identify patterns, anomalies, and potential errors in financial data\n",
        "- Collaborate with finance and accounting teams to understand business requirements and integrate AI solutions into existing workflows\n",
        "- Continuously improve and optimize AI models based on new data and changing business needs\n",
        "\n",
        "Requirements:\n",
        "- Masters or PhD in Computer Science, Artificial Intelligence, or related field\n",
        "- 3+ years of experience developing AI and machine learning solutions, preferably in finance or accounting domains\n",
        "- Strong programming skills in Python and experience with ML frameworks (e.g., TensorFlow, PyTorch, scikit-learn)\n",
        "- Experience with NLP techniques and text analysis\n",
        "- Familiarity with financial systems, ERP software, and accounting principles\n",
        "- Knowledge of data privacy and security best practices\n",
        "- Excellent problem-solving skills and ability to translate complex business requirements into technical solutions\n",
        "\n",
        "No agencies please.\n",
        "This individual must be available for 30h/week and during UK working hours.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fh4UxsGzyvCh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_text = load_and_extract_text(\"CoverLetter/Data/CV_GPT_rev.pdf\")"
      ],
      "metadata": {
        "id": "w9qtW_RXdDwD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "dedzxL_GqtL8",
        "outputId": "b6afab74-2692-4213-9f4e-5e4f1ae9f194"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Itay\\nBen-Dan\\nHaarava\\n20,\\nHerzliya\\nP.\\nO.\\nBox\\n5990,\\nHerzliya,\\nIsrael\\n46100\\nCellular:\\n+972544539284\\nEmail:\\nitaybd@gmail.com\\nProfessional\\nSummary\\nFull\\nStack\\nMachine\\nLearning\\nEngineer\\nwith\\nextensive\\nexperience\\nin\\ndata\\nengineering,\\nmodel\\ndevelopment,\\ndeployment,\\nand\\nmaintenance.\\nExpertise\\nin\\nsports,\\nfinance,\\nand\\nrobotics,\\nwith\\na\\nfocus\\non\\nadvanced\\ndata\\nanalytics\\nand\\nmachine\\nlearning,\\nincluding\\nLLMs\\nand\\nmulti-agent\\nsystems.\\nWork\\nExperience\\n2017-Present:\\nMachine\\nLearning\\nand\\nData\\nScience\\nConsultant\\n●\\nDeveloped\\npredictive\\nmodels\\nfrom\\nscratch,\\nincluding\\ndeployment.\\n●\\nConducted\\nexploratory\\ndata\\nanalysis\\nand\\nadvanced\\nfeature\\ngeneration.\\n●\\nIntegrated\\ndata\\nfrom\\nvarious\\nsources\\ninto\\nunified\\nformats.\\n●\\nDeveloped\\npolicy\\ngenerative\\nmodels\\nusing\\nreinforcement\\nlearning.\\n2017-Present:\\nFounder\\nand\\nHead\\nof\\nResearch,\\nFinzor\\nLtd.\\n●\\nDeveloped\\nportfolio\\nmanagement\\nand\\ninvestment\\nanalysis\\ntools.\\n●\\nLed\\na\\nteam\\nto\\ndeliver\\nsoftware\\nproducts\\nfrom\\ndesign\\nto\\nproduction.\\n2016-2017:\\nPrincipal\\nMachine\\nLearning,\\nPalo\\nAlto\\nNetworks\\n●\\nApplied\\nmachine\\nlearning\\nto\\nreverse\\nengineering\\nand\\nmalware\\nanalysis.\\n●\\nCreated\\nadaptive\\nmodels\\nfor\\nupdating\\nanti-malware\\nfilters\\nand\\nclassifiers.\\n2015-2016:\\nSenior\\nData\\nScientist,\\nSparkBeyond\\n●\\nWorked\\non\\nbig\\ndata\\npredictive\\nanalytics\\nand\\nmachine\\nlearning.\\n●\\nApplied\\nautomatic\\nfeature\\ngeneration\\nmethods\\nto\\ntime\\nseries\\nanalysis\\nand\\nreinforcement\\nlearning.\\n2011-2015:\\nSenior\\nQuantitative\\nResearcher,\\nWorldQuant●\\nDeveloped\\nautomated\\ncomputational\\nmethods\\nfor\\nquant-driven\\nstrategies.\\n●\\nApplied\\npredictive\\nanalytics\\nand\\nstatistical\\nmethods\\nacross\\nvarious\\nassets.\\n2010-2011:\\nSenior\\nSoftware\\nEngineer,\\nBroadcom\\n●\\nDesigned\\nnetwork\\nand\\npacket\\nprocessing\\nmethodologies.\\n●\\nImplemented\\nadvanced\\nautomated\\ntesting\\nmethods.\\n2008-2009:\\nFounder,\\nGenous\\nVision\\n●\\nDeveloped\\nalgorithms\\nfor\\nrecognizing\\nanatomies\\nin\\nultrasound\\nimages.\\nEducation\\n2009:\\nPh.D.\\nin\\nMathematics,\\nTechnion,\\nHaifa\\n●\\nTopic:\\nDiscrete\\nGeometry\\n●\\nSupervisor:\\nDr.\\nRom\\nPinchasi\\n2004:\\nM.Sc.\\nin\\nMathematics,\\nTechnion,\\nHaifa\\n●\\nTopic:\\nGame\\nTheory\\n●\\nThesis:\\nTime\\nSharing\\nunder\\nDichotomous\\nPreferences\\n●\\nGrade:\\n95\\n●\\nSupervisor:\\nProf.\\nRon\\nHolzman\\n2002:\\nB.Sc.\\nin\\nMathematics\\nand\\nComputer\\nScience,\\nTechnion,\\nHaifa\\n●\\nCum\\nLaude\\nSkills\\n●\\nProgramming\\nLanguages:\\nPython,\\nR,\\nC++,\\nJava\\n●\\nMachine\\nLearning\\nFrameworks:\\nTensorFlow,\\nPyTorch,\\nScikit-learn\\n●\\nData\\nAnalysis\\nTools:\\nPandas,\\nNumPy,\\nSQL\\n●\\nOther\\nTechnologies:\\nDocker,\\nKubernetes,\\nAWS,\\nGit,\\nLinux\\nProjects\\nTreeModelVis●\\nVisualization\\ntool\\nfor\\ntree-based\\nmodels,\\naiding\\nin\\nunderstanding\\ndecision\\npaths\\nand\\nfeature\\nimportance.\\n●\\nTechnologies\\nUsed:\\nPython,\\nD3.js\\n●\\nGitHub\\nRepository\\nPublications\\n●\\nPoints\\nwith\\nLarge\\nQuadrant\\nDepth.\\nJoCG\\n2(1):\\n128-143\\n(2011)\\n●\\nPoints\\nwith\\nlarge\\nquadrant\\ndepth.\\nSymposium\\non\\nComputational\\nGeometry\\n2010:\\n358-364\\n●\\nOn\\na\\nproblem\\nabout\\nquadrant\\ndepth.\\nComput.\\nGeom.\\n43(6-7):\\n587-592\\n(2010)\\n●\\nPoints\\nwith\\nlarge\\nalphadepth.\\nJ.\\nComb.\\nTheory,\\nSer.\\nA\\n116(3):\\n747-755\\n(2009)\\nContact\\nInformation\\n●\\nEmail:\\nitaybd@gmail.com\\n●\\nLinkedIn:\\nItay\\nBen-Dan\\nLinkedIn\\nProfile\\nThis\\nCV\\nis\\nconcise\\nand\\ntailored,\\nhighlighting\\nkey\\nskills\\nand\\nexperience\\nin\\na\\nclean,\\norganized\\nformat.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_cover_letter(model,cv_text, job_description).text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "xO8AqhoydojT",
        "outputId": "5e4cc8dc-5b2d-4a1b-e104-6ecc6444be9c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HERE\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dear Hiring Manager,\\n\\nMy experience developing AI solutions, particularly in finance and accounting, aligns well with your requirements.  I have successfully implemented predictive models from scratch, integrated data from diverse sources, and created intelligent systems to automate tasks.  My expertise in NLP, ML frameworks like TensorFlow and PyTorch, and financial systems, coupled with a strong understanding of data privacy and security best practices, makes me a suitable candidate for this position. \\n\\nBest Regards, \\n\\nItay Ben-Dan \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_key_phrases(model, cover_letter_text, job_description_text, num_phrases=4):\n",
        "    \"\"\"\n",
        "    Extracts key phrases from a cover letter based on their relevance to a job description using Gemini.\n",
        "\n",
        "    Args:\n",
        "        model: The Gemini language model instance.\n",
        "        cover_letter_text: The text of the cover letter.\n",
        "        job_description_text: The text of the job description.\n",
        "        num_phrases: The number of phrases to extract (default is 4).\n",
        "\n",
        "    Returns:\n",
        "        A list of the most important phrases.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"Given the following cover letter and job description, identify the {num_phrases} most important words or two-word expressions in the cover letter that are most relevant to the job description.\n",
        "    These words or Two words expression should show professional skills of the candidate they should reflecy the candidate as a doer, if it is an action like developing/programming it should be accompanied with the subject of the action. such as :\n",
        "    developing AI models.\n",
        "\n",
        "    Cover Letter:\n",
        "    {cover_letter_text}\n",
        "\n",
        "    Job Description:\n",
        "    {job_description_text}\n",
        "    \"\"\"\n",
        "    print(\"HERE\")\n",
        "    response = model.generate_content(prompt)\n",
        "\n",
        "    # Extract key phrases from response\n",
        "    important_phrases = response.text.split('\\n')  # Assuming each phrase is on a new line\n",
        "    return important_phrases"
      ],
      "metadata": {
        "id": "hCqxY9clh-rH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time.sleep(3)\n",
        "job_description = job_description #load_and_extract_text(\"job_description.pdf\")\n",
        "cv_text = load_and_extract_text(\"CoverLetter/Data/CV_GPT_rev.pdf\")\n",
        "cover_letter_text = generate_cover_letter(model,cv_text, job_description).text\n",
        "for k in cover_letter_text.split(\".\"):\n",
        "  print(k)\n",
        "time.sleep(3)\n",
        "key_phrases = extract_key_phrases(model, cover_letter_text, job_description, num_phrases=3)\n",
        "for k in key_phrases:\n",
        "  print(\"#\"*30)\n",
        "  print(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "id": "CC26_KnJNl7k",
        "outputId": "265269f0-11ee-49c8-bd21-fb0b72732e62"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HERE\n",
            "Dear Hiring Manager,\n",
            "\n",
            "I am writing to express my strong interest in the AI Developer position at your company\n",
            " My extensive experience in developing and deploying machine learning solutions, particularly in the finance domain, aligns perfectly with your requirements\n",
            " I have a proven track record of success in automating financial processes, including invoice reconciliation, and am proficient in Python, TensorFlow, PyTorch, and NLP techniques\n",
            " I am confident I can contribute significantly to your team and deliver impactful AI solutions to optimize your invoice reconciliation process\n",
            "\n",
            "\n",
            "Best regards,\n",
            "\n",
            "Itay Ben-Dan \n",
            "\n",
            "HERE\n",
            "##############################\n",
            "Here are the 3 most important words/phrases from the cover letter that align with the job description, along with their connection to the job's requirements:\n",
            "##############################\n",
            "\n",
            "##############################\n",
            "1. **Developing machine learning solutions:** This directly connects to the job's core responsibility of \"Develop and implement AI and machine learning models to automate invoice reconciliation.\" \n",
            "##############################\n",
            "2. **Automating financial processes:** This highlights the candidate's experience in a specific area the job requires (improving invoice reconciliation) and demonstrates their ability to \"create intelligent systems to unify diverse invoice data into a standardized format.\"\n",
            "##############################\n",
            "3. **NLP techniques:** This matches the job description's requirement for \"Design and build natural language processing (NLP) solutions to extract key information from unstructured invoice data.\"\n",
            "##############################\n",
            "\n",
            "##############################\n",
            "These words and phrases showcase the candidate's relevant skills and experience, making them stand out as a strong candidate for the AI Developer role. \n",
            "##############################\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_strings(key_phrases):\n",
        "  R= []\n",
        "  for k in key_phrases:\n",
        "    A = (re.findall('\\*\\*.*?\\*\\*',k))\n",
        "    if len(A)>0:\n",
        "      R += re.findall('[A-Za-z\\ 0-9]+',A[0])\n",
        "  return R"
      ],
      "metadata": {
        "id": "ZPDpn1jp0eS1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_strings(extract_key_phrases(model, cover_letter_text, job_description, num_phrases=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "5IeUwuSB4tJx",
        "outputId": "39d39d4e-32d2-46ea-8c9b-d574bf016dca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HERE\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Developing AI models', 'Automating financial processes', 'NLP techniques']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.oxml.ns import qn\n",
        "from docx.oxml import OxmlElement\n",
        "\n",
        "def bold_strings_in_text(text, bold_strings, output_doc):\n",
        "    # Create a new Document\n",
        "    doc = Document()\n",
        "\n",
        "    # Add the text to the document, with specified strings in bold\n",
        "    para = doc.add_paragraph()\n",
        "    parts = split_text(text, bold_strings)\n",
        "\n",
        "    for part in parts:\n",
        "        if part.lower() in [s.lower() for s in bold_strings]:\n",
        "            run = para.add_run(part)\n",
        "            run.bold = True\n",
        "        else:\n",
        "            para.add_run(part)\n",
        "\n",
        "    # Save the document\n",
        "    doc.save(output_doc)\n",
        "\n",
        "def split_text(text, bold_strings):\n",
        "    import re\n",
        "    pattern = '|'.join(re.escape(s) for s in bold_strings)\n",
        "    parts = re.split(f'({pattern})', text)\n",
        "    print(parts)\n",
        "    return parts\n",
        "\n",
        "# Example usage\n",
        "text = \"This is a sample text. We want some words like sample and words to be bold.\"\n",
        "bold_strings = [\"sample\", \"words\"]\n",
        "bold_strings_in_text(text, bold_strings,\"output.docx\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpV3EAEf5AQG",
        "outputId": "25a99b7f-9c40-4384-ec1e-64d93f61edea"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a ', 'sample', ' text. We want some ', 'words', ' like ', 'sample', ' and ', 'words', ' to be bold.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cl_text = generate_cover_letter(model,cv_text, job_description).text\n",
        "strings = extract_strings(extract_key_phrases(model, cl_text, job_description, num_phrases=3))\n",
        "bold_strings_in_text(cl_text, strings, \"test2.docx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "Tqeq5Cgn5otS",
        "outputId": "156738de-f746-415c-cb3e-d039e3665025"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HERE\n",
            "HERE\n",
            "['Dear Hiring Manager,\\n\\nMy extensive experience in developing and deploying AI and machine learning solutions, particularly in finance, aligns perfectly with your requirements. I have a proven track record of building predictive models, integrating data from various sources, and applying ', 'NLP techniques for data extraction', ', as demonstrated in my work at Finzor Ltd and Palo Alto Networks.  My proficiency in Python, TensorFlow, PyTorch, and NLP makes me well-suited for this role. I am available to work 30 hours per week during UK working hours. \\n\\nBest regards,\\n\\nItay Ben-Dan \\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCzpElqwdp_n",
        "outputId": "1d844a2f-7137-4cf4-c41d-0e067577399d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Developing AI models', 'NLP techniques for data extraction', 'Integrating data from various sources']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cl_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "37GpRjmhHvFW",
        "outputId": "ef558bba-9f60-4734-ee25-6599c135325f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dear Hiring Manager,\\n\\nMy extensive experience in developing and deploying AI and machine learning solutions, particularly in finance, aligns perfectly with your requirements. I have a proven track record of building predictive models, integrating data from various sources, and applying NLP techniques for data extraction, as demonstrated in my work at Finzor Ltd and Palo Alto Networks.  My proficiency in Python, TensorFlow, PyTorch, and NLP makes me well-suited for this role. I am available to work 30 hours per week during UK working hours. \\n\\nBest regards,\\n\\nItay Ben-Dan \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxWAcOH0dcDX",
        "outputId": "f4cff766-b8ca-48aa-b853-b7dd822589f9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CoverLetter  output.docx  sample_data  test2.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx reportlab pypandoc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfIQHI25AyE2",
        "outputId": "7dfa7576-3eb4-4415-8758-29173ba06c26"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Collecting reportlab\n",
            "  Downloading reportlab-4.2.2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.13-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from reportlab) (9.4.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from reportlab) (5.2.0)\n",
            "Downloading reportlab-4.2.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypandoc-1.13-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: reportlab, pypandoc\n",
            "Successfully installed pypandoc-1.13 reportlab-4.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "import re\n",
        "import pypandoc\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas"
      ],
      "metadata": {
        "id": "asVLIGH_CVwd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCfE-HPko4M3",
        "outputId": "684382c7-1019-4c88-b105-4f9256d39ab0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx2pdf\n",
            "  Downloading docx2pdf-0.1.8-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from docx2pdf) (4.66.5)\n",
            "Downloading docx2pdf-0.1.8-py3-none-any.whl (6.7 kB)\n",
            "Installing collected packages: docx2pdf\n",
            "Successfully installed docx2pdf-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "import re\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "from io import BytesIO\n"
      ],
      "metadata": {
        "id": "5WKdksutokW3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_docx_to_pdf(input_docx, output_pdf, title, name):\n",
        "    # Convert docx to PDF using BytesIO to avoid creating temporary files\n",
        "    pdf_buffer = BytesIO()\n",
        "    pdf = canvas.Canvas(pdf_buffer, pagesize=letter)\n",
        "    width, height = letter\n",
        "\n",
        "    # Add title and name\n",
        "    pdf.setFont(\"Helvetica-Bold\", 16)\n",
        "    pdf.drawString(72, height - 72, title)\n",
        "    pdf.setFont(\"Helvetica\", 12)\n",
        "    pdf.drawString(72, height - 92, name)\n",
        "\n",
        "    # Read the docx file and add content to the PDF\n",
        "    doc = Document(input_docx)\n",
        "    text_object = pdf.beginText(72, height - 112)\n",
        "    text_object.setFont(\"Helvetica\", 10)\n",
        "\n",
        "    for para in doc.paragraphs:\n",
        "        for run in para.runs:\n",
        "            if run.bold:\n",
        "                text_object.setFont(\"Helvetica-Bold\", 10)\n",
        "            else:\n",
        "                text_object.setFont(\"Helvetica\", 10)\n",
        "            text_object.textLine(run.text)\n",
        "\n",
        "    pdf.drawText(text_object)\n",
        "    pdf.save()"
      ],
      "metadata": {
        "id": "rQjyQehNCYX3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = \"Itay Ben-Dan\"\n",
        "title = \"AI Developer Application\"\n",
        "cl_pdf_name = \"cl.pdf\"\n",
        "convert_docx_to_pdf(output_doc_name, cl_pdf_name, title, name)"
      ],
      "metadata": {
        "id": "k1bGwb0pCpsM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.units import inch\n",
        "import re\n",
        "\n",
        "def create_pdf_with_bold_strings(output_pdf, title, name, text, bold_strings):\n",
        "    # Create the PDF document\n",
        "    pdf = SimpleDocTemplate(output_pdf, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    story = []\n",
        "\n",
        "    # Define custom styles for title and body text\n",
        "    title_style = ParagraphStyle(name='Title', fontSize=16, leading=20, spaceAfter=20, alignment=1)\n",
        "    name_style = ParagraphStyle(name='Name', fontSize=12, leading=14, spaceAfter=14, alignment=1)\n",
        "    body_style = styles['BodyText']\n",
        "    bold_style = ParagraphStyle(name='Bold', parent=styles['BodyText'], fontName='Helvetica-Bold')\n",
        "\n",
        "    # Add title\n",
        "    story.append(Paragraph(title, title_style))\n",
        "\n",
        "    # Add name\n",
        "    story.append(Paragraph(name, name_style))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    # Process and add the main text with bold strings\n",
        "    parts = split_text(text, bold_strings)\n",
        "    for part in parts:\n",
        "        if any(re.fullmatch(re.escape(b), part, re.IGNORECASE) for b in bold_strings):\n",
        "            story.append(Paragraph(part, bold_style))\n",
        "        else:\n",
        "            story.append(Paragraph(part, body_style))\n",
        "\n",
        "    # Build the PDF\n",
        "    pdf.build(story)\n",
        "\n",
        "def split_text(text, bold_strings):\n",
        "    pattern = '|'.join(re.escape(s) for s in bold_strings)\n",
        "    parts = re.split(f'({pattern})', text, flags=re.IGNORECASE)\n",
        "    return parts\n",
        "\n",
        "# Example usage\n",
        "cv_text = \"John Doe\"\n",
        "text = \"\"\"Dear Hiring Manager,\n",
        "\n",
        "I am writing to express my strong interest in the AI Developer position at your company. My extensive experience in developing and deploying AI solutions, particularly in finance and data analytics, aligns well with your requirements. My accomplishments include building predictive models from scratch, integrating data from diverse sources into unified formats, and creating adaptive models for updating anti-malware filters. I am confident that my skills and experience would be a valuable asset to your team.\n",
        "\"\"\"\n",
        "bold_strings = [\"developing and deploying AI solutions\", \"integrating data from diverse sources\", \"creating adaptive models\"]\n",
        "output_pdf = \"output_professional.pdf\"\n",
        "title = \"AI Developer Application\"\n",
        "\n",
        "create_pdf_with_bold_strings(output_pdf, title, cv_text, text, bold_strings)\n"
      ],
      "metadata": {
        "id": "V5Nww8qVsII_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!git clone https://github.com/openai/openai-quickstart-node.git\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install python-dotenv\n",
        "import os\n",
        "os.chdir(\"openai-quickstart-node\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "J7_M39b1Nhmh",
        "outputId": "9ea7c8a0-e791-4a91-9da5-f371b04644e1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!git clone https://github.com/openai/openai-quickstart-node.git\\n!pip install openai\\n!pip install tiktoken\\n!pip install python-dotenv\\nimport os\\nos.chdir(\"openai-quickstart-node\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUboZeAQMVUc",
        "outputId": "ee777e55-1092-47b1-9387-8b50b805c0cd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import sys\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY2')\n",
        "\n",
        "openai.api_key  = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "rFaLFWH-5a4y"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from docx import Document\n",
        "import re\n",
        "\n",
        "# Set your OpenAI API key\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Function to generate improved text using OpenAI API\n",
        "def generate_improved_text(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "# Function to improve the cover letter\n",
        "def improve_cover_letter(cover_letter, critique):\n",
        "    prompt = f\"Given the following cover letter and critique, generate an improved version of the cover letter:\\n\\nCover Letter:\\n{cover_letter}\\n\\nCritique:\\n{critique}\\n\\nImproved Cover Letter:\"\n",
        "    return generate_improved_text(prompt)\n",
        "\n",
        "# Function to improve the CV\n",
        "def improve_cv(cv_text, critique):\n",
        "    prompt = f\"Given the following CV and critique, generate an improved version of the CV:\\n\\nCV:\\n{cv_text}\\n\\nCritique:\\n{critique}\\n\\nImproved CV:\"\n",
        "    return generate_improved_text(prompt)\n",
        "\n",
        "# Function to create a PDF from text\n",
        "def create_pdf(output_pdf, title, name, text):\n",
        "    pdf = SimpleDocTemplate(output_pdf, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    story = []\n",
        "\n",
        "    # Define custom styles for title and body text\n",
        "    title_style = ParagraphStyle(name='Title', fontSize=16, leading=20, spaceAfter=20, alignment=1)\n",
        "    name_style = ParagraphStyle(name='Name', fontSize=12, leading=14, spaceAfter=14, alignment=1)\n",
        "    body_style = styles['BodyText']\n",
        "    bold_style = ParagraphStyle(name='Bold', parent=styles['BodyText'], fontName='Helvetica-Bold')\n",
        "\n",
        "    # Add title\n",
        "    story.append(Paragraph(title, title_style))\n",
        "\n",
        "    # Add name\n",
        "    story.append(Paragraph(name, name_style))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    # Process and add the main text with bold strings\n",
        "    parts = split_text(text, [])\n",
        "    for part in parts:\n",
        "        if any(re.fullmatch(re.escape(b), part, re.IGNORECASE) for b in []):\n",
        "            story.append(Paragraph(part, bold_style))\n",
        "        else:\n",
        "            story.append(Paragraph(part, body_style))\n",
        "\n",
        "    # Build the PDF\n",
        "    pdf.build(story)\n",
        "\n",
        "# Function to split text\n",
        "def split_text(text, bold_strings):\n",
        "    pattern = '|'.join(re.escape(s) for s in bold_strings)\n",
        "    parts = re.split(f'({pattern})', text, flags=re.IGNORECASE)\n",
        "    return parts\n",
        "\n",
        "# Example usage\n",
        "original_cover_letter = \"\"\"Dear Hiring Manager,\n",
        "\n",
        "I am writing to express my strong interest in the AI Developer position at your company. My extensive experience in developing and deploying AI solutions, particularly in finance and data analytics, aligns well with your requirements. My accomplishments include building predictive models from scratch, integrating data from diverse sources into unified formats, and creating adaptive models for updating anti-malware filters. I am confident that my skills and experience would be a valuable asset to your team.\n",
        "\"\"\"\n",
        "cover_letter_critique = \"\"\"Strong Points: Relevance, Specific Accomplishments, Professional Tone.\n",
        "Weak Points: Lack of Specifics, Formatting, Connection to Job Description.\n",
        "Suggested Improvements: Include Specific Technologies, Better Formatting, Tailor to Job Description.\n",
        "Rating: 6/10\n",
        "Probability of Follow-Up Contact: 0.4 (40%)\n",
        "\"\"\"\n",
        "\n",
        "original_cv = \"\"\"Itay Ben-Dan\n",
        "Haarava 20, Herzliya\n",
        "P. O. Box 5990, Herzliya, Israel 46100\n",
        "Cellular: +972544539284\n",
        "Email: itaybd@gmail.com\n",
        "\n",
        "Professional Summary\n",
        "Full Stack Machine Learning Engineer with extensive experience in data engineering, model development, deployment, and maintenance. Expertise in sports, finance, and robotics, with a focus on advanced data analytics and machine learning, including LLMs and multi-agent systems.\n",
        "\n",
        "Work Experience\n",
        "2017-Present: Machine Learning and Data Science Consultant\n",
        "• Developed predictive models from scratch, including deployment.\n",
        "• Conducted exploratory data analysis and advanced feature generation.\n",
        "• Integrated data from various sources into unified formats.\n",
        "• Developed policy generative models using reinforcement learning.\n",
        "\n",
        "2017-2020: Founder and Head of Research, Finzor Ltd.\n",
        "• Developed portfolio management and investment analysis tools.\n",
        "• Led a team to deliver software products from design to production.\n",
        "\n",
        "2016-2017: Principal Machine Learning, Palo Alto Networks\n",
        "• Applied machine learning to reverse engineering and malware analysis.\n",
        "• Created adaptive models for updating anti-malware filters and classifiers.\n",
        "\n",
        "2015-2016: Senior Data Scientist, SparkBeyond\n",
        "• Worked on big data predictive analytics and machine learning.\n",
        "• Applied automatic feature generation methods to time series analysis and reinforcement learning.\n",
        "\n",
        "2011-2015: Senior Quantitative Researcher, WorldQuant\n",
        "• Developed automated computational methods for quant-driven strategies.\n",
        "• Applied predictive analytics and statistical methods across various assets.\n",
        "\n",
        "2010-2011: Senior Software Engineer, Broadcom\n",
        "• Designed network and packet processing methodologies.\n",
        "• Implemented advanced automated testing methods.\n",
        "\n",
        "2008-2009: Founder, Genous Vision\n",
        "• Developed algorithms for recognizing anatomies in ultrasound images.\n",
        "\n",
        "Education\n",
        "2009: Ph.D. in Mathematics, Technion, Haifa\n",
        "• Topic: Discrete Geometry\n",
        "• Supervisor: Dr. Rom Pinchasi\n",
        "\n",
        "2004: M.Sc. in Mathematics, Technion, Haifa\n",
        "• Topic: Game Theory\n",
        "• Thesis: Time Sharing under Dichotomous Preferences\n",
        "• Grade: 95\n",
        "• Supervisor: Prof. Ron Holzman\n",
        "\n",
        "2002: B.Sc. in Mathematics and Computer Science, Technion, Haifa\n",
        "• Cum Laude\n",
        "\n",
        "Skills\n",
        "• Programming Languages: Python, R, C++, Java\n",
        "• Machine Learning Frameworks: TensorFlow, PyTorch, Scikit-learn\n",
        "• Data Analysis Tools: Pandas, NumPy, SQL\n",
        "• Other Technologies: Docker, Kubernetes, AWS, Git, Linux\n",
        "\n",
        "Projects\n",
        "TreeModelVis\n",
        "• Visualization tool for tree-based models, aiding in understanding decision paths and feature importance.\n",
        "• Technologies Used: Python, D3.js\n",
        "• GitHub Repository\n",
        "\n",
        "Publications\n",
        "• Points with Large Quadrant Depth. JoCG 2(1): 128-143 (2011)\n",
        "• Points with large quadrant depth. Symposium on Computational Geometry 2010: 358-364\n",
        "• On a problem about quadrant depth. Comput. Geom. 43(6-7): 587-592 (2010)\n",
        "• Points with large alphadepth. J. Comb. Theory, Ser. A 116(3): 747-755 (2009)\n",
        "\n",
        "Contact Information\n",
        "• Email: itaybd@gmail.com\n",
        "• LinkedIn: Itay Ben-Dan LinkedIn Profile\n",
        "\"\"\"\n",
        "\n",
        "cv_critique = \"\"\"Strong Points: Comprehensive Experience, Relevant Roles, Educational Background.\n",
        "Weak Points: Length and Density, Lack of Specific Results, Skills Section.\n",
        "Suggested Improvements: Highlight Key Accomplishments, Focus on Relevance, Skills Section.\n",
        "Rating: 7/10\n",
        "Probability of Follow-Up Contact: 0.5 (50%)\n",
        "\"\"\"\n",
        "\n",
        "improved_cover_letter = improve_cover_letter(original_cover_letter, cover_letter_critique)\n",
        "improved_cv = improve_cv(original_cv, cv_critique)\n",
        "\n",
        "# Save improved cover letter as PDF\n",
        "create_pdf(\"improved_cover_letter.pdf\", \"AI Developer Application\", \"John Doe\", improved_cover_letter)\n",
        "\n",
        "# Save improved CV as PDF\n",
        "create_pdf(\"improved_cv.pdf\", \"Curriculum Vitae\", \"Itay Ben-Dan\", improved_cv)\n"
      ],
      "metadata": {
        "id": "K4Ksn0DwPbhW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "\n",
        "# Set your OpenAI API key\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Function to generate improved text using OpenAI API\n",
        "def generate_improved_text(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "# Function to improve the cover letter\n",
        "def improve_cover_letter(cover_letter, critique):\n",
        "    prompt = f\"Given the following cover letter and critique, generate an improved version of the cover letter:\\n\\nCover Letter:\\n{cover_letter}\\n\\nCritique:\\n{critique}\\n\\nImproved Cover Letter:\"\n",
        "    return generate_improved_text(prompt)\n",
        "\n",
        "# Function to improve the CV\n",
        "def improve_cv(cv_text, critique):\n",
        "    prompt = f\"Given the following CV and critique, generate an improved version of the CV:\\n\\nCV:\\n{cv_text}\\n\\nCritique:\\n{critique}\\n\\nImproved CV:\"\n",
        "    return generate_improved_text(prompt)\n",
        "\n",
        "# Function to create a PDF from text\n",
        "def create_pdf(output_pdf, title, name, text):\n",
        "    pdf = SimpleDocTemplate(output_pdf, pagesize=letter)\n",
        "    styles = getSampleStyleSheet()\n",
        "    story = []\n",
        "\n",
        "    # Define custom styles for title and body text\n",
        "    title_style = ParagraphStyle(name='Title', fontSize=16, leading=20, spaceAfter=20, alignment=1)\n",
        "    name_style = ParagraphStyle(name='Name', fontSize=12, leading=14, spaceAfter=14, alignment=1)\n",
        "    body_style = styles['BodyText']\n",
        "    bold_style = ParagraphStyle(name='Bold', parent=styles['BodyText'], fontName='Helvetica-Bold')\n",
        "\n",
        "    # Add title\n",
        "    story.append(Paragraph(title, title_style))\n",
        "\n",
        "    # Add name\n",
        "    story.append(Paragraph(name, name_style))\n",
        "    story.append(Spacer(1, 12))\n",
        "\n",
        "    # Process and add the main text\n",
        "    paragraphs = text.split(\"\\n\\n\")\n",
        "    for para in paragraphs:\n",
        "        story.append(Paragraph(para.replace('\\n', '<br/>'), body_style))\n",
        "        story.append(Spacer(1, 12))\n",
        "\n",
        "    # Build the PDF\n",
        "    pdf.build(story)\n",
        "\n",
        "# Example usage\n",
        "original_cover_letter = \"\"\"Dear Hiring Manager,\n",
        "\n",
        "I am writing to express my strong interest in the AI Developer position at your company. My extensive experience in developing and deploying AI solutions, particularly in finance and data analytics, aligns well with your requirements. My accomplishments include building predictive models from scratch, integrating data from diverse sources into unified formats, and creating adaptive models for updating anti-malware filters. I am confident that my skills and experience would be a valuable asset to your team.\n",
        "\"\"\"\n",
        "cover_letter_critique = \"\"\"Strong Points: Relevance, Specific Accomplishments, Professional Tone.\n",
        "Weak Points: Lack of Specifics, Formatting, Connection to Job Description.\n",
        "Suggested Improvements: Include Specific Technologies, Better Formatting, Tailor to Job Description.\n",
        "Rating: 6/10\n",
        "Probability of Follow-Up Contact: 0.4 (40%)\n",
        "\"\"\"\n",
        "\n",
        "original_cv = \"\"\"Itay Ben-Dan\n",
        "Haarava 20, Herzliya\n",
        "P. O. Box 5990, Herzliya, Israel 46100\n",
        "Cellular: +972544539284\n",
        "Email: itaybd@gmail.com\n",
        "\n",
        "Professional Summary\n",
        "Full Stack Machine Learning Engineer with extensive experience in data engineering, model development, deployment, and maintenance. Expertise in sports, finance, and robotics, with a focus on advanced data analytics and machine learning, including LLMs and multi-agent systems.\n",
        "\n",
        "Work Experience\n",
        "2017-Present: Machine Learning and Data Science Consultant\n",
        "• Developed predictive models from scratch, including deployment.\n",
        "• Conducted exploratory data analysis and advanced feature generation.\n",
        "• Integrated data from various sources into unified formats.\n",
        "• Developed policy generative models using reinforcement learning.\n",
        "\n",
        "2017-2020: Founder and Head of Research, Finzor Ltd.\n",
        "• Developed portfolio management and investment analysis tools.\n",
        "• Led a team to deliver software products from design to production.\n",
        "\n",
        "2016-2017: Principal Machine Learning, Palo Alto Networks\n",
        "• Applied machine learning to reverse engineering and malware analysis.\n",
        "• Created adaptive models for updating anti-malware filters and classifiers.\n",
        "\n",
        "2015-2016: Senior Data Scientist, SparkBeyond\n",
        "• Worked on big data predictive analytics and machine learning.\n",
        "• Applied automatic feature generation methods to time series analysis and reinforcement learning.\n",
        "\n",
        "2011-2015: Senior Quantitative Researcher, WorldQuant\n",
        "• Developed automated computational methods for quant-driven strategies.\n",
        "• Applied predictive analytics and statistical methods across various assets.\n",
        "\n",
        "2010-2011: Senior Software Engineer, Broadcom\n",
        "• Designed network and packet processing methodologies.\n",
        "• Implemented advanced automated testing methods.\n",
        "\n",
        "2008-2009: Founder, Genous Vision\n",
        "• Developed algorithms for recognizing anatomies in ultrasound images.\n",
        "\n",
        "Education\n",
        "2009: Ph.D. in Mathematics, Technion, Haifa\n",
        "• Topic: Discrete Geometry\n",
        "• Supervisor: Dr. Rom Pinchasi\n",
        "\n",
        "2004: M.Sc. in Mathematics, Technion, Haifa\n",
        "• Topic: Game Theory\n",
        "• Thesis: Time Sharing under Dichotomous Preferences\n",
        "• Grade: 95\n",
        "• Supervisor: Prof. Ron Holzman\n",
        "\n",
        "2002: B.Sc. in Mathematics and Computer Science, Technion, Haifa\n",
        "• Cum Laude\n",
        "\n",
        "Skills\n",
        "• Programming Languages: Python, R, C++, Java\n",
        "• Machine Learning Frameworks: TensorFlow, PyTorch, Scikit-learn\n",
        "• Data Analysis Tools: Pandas, NumPy, SQL\n",
        "• Other Technologies: Docker, Kubernetes, AWS, Git, Linux\n",
        "\n",
        "Projects\n",
        "TreeModelVis\n",
        "• Visualization tool for tree-based models, aiding in understanding decision paths and feature importance.\n",
        "• Technologies Used: Python, D3.js\n",
        "• GitHub Repository\n",
        "\n",
        "Publications\n",
        "• Points with Large Quadrant Depth. JoCG 2(1): 128-143 (2011)\n",
        "• Points with large quadrant depth. Symposium on Computational Geometry 2010: 358-364\n",
        "• On a problem about quadrant depth. Comput. Geom. 43(6-7): 587-592 (2010)\n",
        "• Points with large alphadepth. J. Comb. Theory, Ser. A 116(3): 747-755 (2009)\n",
        "\n",
        "Contact Information\n",
        "• Email: itaybd@gmail.com\n",
        "• LinkedIn: Itay Ben-Dan LinkedIn Profile\n",
        "\"\"\"\n",
        "\n",
        "cv_critique = \"\"\"Strong Points: Comprehensive Experience, Relevant Roles, Educational Background.\n",
        "Weak Points: Length and Density, Lack of Specific Results, Skills Section.\n",
        "Suggested Improvements: Highlight Key Accomplishments, Focus on Relevance, Skills Section.\n",
        "Rating: 7/10\n",
        "Probability of Follow-Up Contact: 0.5 (50%)\n",
        "\"\"\"\n",
        "\n",
        "improved_cover_letter = improve_cover_letter(original_cover_letter, cover_letter_critique)\n",
        "improved_cv = improve_cv(original_cv, cv_critique)\n",
        "\n",
        "# Save improved cover letter as PDF\n",
        "create_pdf(\"improved_cover_letter.pdf\", \"AI Developer Application\", \"John Doe\", improved_cover_letter)\n",
        "\n",
        "# Save improved CV as PDF\n",
        "create_pdf(\"improved_cv.pdf\", \"Curriculum Vitae\", \"Itay Ben-Dan\", improved_cv)\n"
      ],
      "metadata": {
        "id": "fJTJ--QxtJRz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate_cover_letter\n",
        "#extract_key_phrases\n",
        "#create_pdf_with_bold_strings(output_pdf, title, cv_text, text, bold_strings)\n",
        "#improve_cover_letter\n",
        "#improved_cover_letter\n",
        "cover_letter_text = generate_cover_letter(model,cv_text, job_description).text\n",
        "#extract_key_phrases(model, cover_letter_text, job_description, num_phrases=3)\n",
        "bold_strings = extract_strings(extract_key_phrases(model, cl_text, job_description, num_phrases=3))\n",
        "create_pdf_with_bold_strings(output_pdf, title, cv_text, text, bold_strings)\n",
        "improved_cover_letter = improve_cover_letter(original_cover_letter, cover_letter_critique)\n",
        "improved_cv = improve_cv(original_cv, cv_critique)\n"
      ],
      "metadata": {
        "id": "Z5Gq7NGyUIUd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ja6LUZQSmf6h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}